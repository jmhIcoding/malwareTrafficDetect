#coding:utf8
__author__ = 'jmh081701'
import tensorflow as tf
import numpy as np

train_rate=0.0001 #学习速率
train_step=100000 #使用提前终止的正则化方式
batch_size=64
pkt_num=16        #每次都输入pkt_num个数据包进来
pkt_length=128    #每个数据包就只取前128个byte

frame_size=16  #每一帧都16个字节,不足的使用0补上
sequence_length=128 #

hidden_num=5    #网络深度为5
n_classes=256   #每个字节可以取到0-255
#定义输入、输出
x=tf.placeholder(dtype=tf.float32,shape=[None,pkt_length*pkt_num],name="inputx")
y=tf.placeholder(dtype=tf.float32,shape=[None,n_classes],name="outputy")
#注意！y是采样出来的！！！！需要单独进行输入输出的处理
#权重
weight=tf.Variable(tf.truncated_normal(shape=[hidden_num,n_classes],mean=0.0001,stddev=0.01))
bias=tf.Variable(tf.zeros(shape=[n_classes]))
def RNN(x,weight,bias):
    x=tf.reshape(shape=[-1,sequence_length,frame_size])
    rnn_cell=tf.nn.rnn_cell.BasicRNNCell(hidden_num)
    output,states=tf.nn.dynamic_rnn(rnn_cell,x,dtype=tf.float32)
    return tf.nn.softmax(tf.matmul(output[:,-1,:],weight)+bias,1)
predy=RNN(x,weight,bias)
cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predy,labels=y))
train=tf.train.AdadeltaOptimizer(learning_rate=train_rate).minimize(cost)

correct_pred=tf.equal(tf.arg_max(predy,1),tf.argmax(y,1))
accuracy=tf.reduce_mean(tf.to_float(correct_pred))
sess=tf.Session()
sess.run(tf.initialize_all_variables())
step=1

while step<train_step:
    #装备数据
    #计算
